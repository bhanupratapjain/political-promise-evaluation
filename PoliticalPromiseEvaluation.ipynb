{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import operator\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import twitter\n",
    "\n",
    "import newspaper\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from nytimesarticle import articleAPI\n",
    "\n",
    "from twitter import read_token_file, oauth_dance, write_token_file\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "\n",
    "from miners.NewsMiner import NewsMiner\n",
    "from miners.TwitterMiner import TwitterMiner\n",
    "\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class TwitterMiner:\n",
    "    def __init__(self):\n",
    "        \"\"\n",
    "\n",
    "    def login(self):\n",
    "        APP_NAME = 'PPE'\n",
    "        CONSUMER_KEY = 'JK9vbq72nrL8BwBBUhOKASosL'\n",
    "        CONSUMER_SECRET = 'tQvz6zTa3nMbLMISPRMjFW6UDIpUBoDjMoFqg3zCI9SuoQrEHT'\n",
    "        TOKEN_FILE = 'out/twitter.oauth'\n",
    "\n",
    "        try:\n",
    "            (oauth_token, oauth_token_secret) = read_token_file(TOKEN_FILE)\n",
    "        except IOError as e:\n",
    "            (oauth_token, oauth_token_secret) = oauth_dance(APP_NAME, CONSUMER_KEY,\n",
    "                                                            CONSUMER_SECRET)\n",
    "\n",
    "            if not os.path.isdir('out'):\n",
    "                os.mkdir('out')\n",
    "\n",
    "            write_token_file(TOKEN_FILE, oauth_token, oauth_token_secret)\n",
    "\n",
    "        return twitter.Twitter(domain='api.twitter.com', api_version='1.1',\n",
    "                               auth=twitter.oauth.OAuth(oauth_token, oauth_token_secret,\n",
    "                                                        CONSUMER_KEY, CONSUMER_SECRET))\n",
    "\n",
    "def get_tweets():\n",
    "    return TwitterMiner().login().search.tweets(q=\"#trump\", count=100)\n",
    "\n",
    "\n",
    "def get_articles():\n",
    "    nm = NewsMiner()\n",
    "    articles = nm.get_articles(\"Donald Trump\", \"20170120\", \"20170830\")\n",
    "    nm.get_text(articles)\n",
    "    with open('out/nyt_articles.json', 'w') as fout:\n",
    "        json.dump(articles, fout)\n",
    "    print(articles)\n",
    "\n",
    "    # cnn = NewsMiner().sources['cnn']\n",
    "    # for i in range(10):\n",
    "    #     print (cnn.size())\n",
    "\n",
    "    \n",
    "class NewsMiner:\n",
    "    def __init__(self):\n",
    "        self.client = newspaper\n",
    "        self.sources = {}\n",
    "        self.article_api = articleAPI('60425d8974b1484692c368f8c52e4c1f')\n",
    "        self.nyi_api_key = \"60425d8974b1484692c368f8c52e4c1f\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.get_articles()\n",
    "        # self.add_source()\n",
    "\n",
    "    def add_source(self):\n",
    "        self.sources['cnn'] = self.client.build('http://cnn.com', memoize_articles=False)\n",
    "\n",
    "    def get_articles(self, search_term, begin_date, end_date):\n",
    "        articles = []\n",
    "        url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "        for page in range(100):\n",
    "            queries = {\n",
    "                'api-key': self.nyi_api_key,\n",
    "                'q': search_term,\n",
    "                'begin_date': begin_date,\n",
    "                'end_date': end_date,\n",
    "                'page': page\n",
    "            }\n",
    "            req_t = requests.get(url, params=queries)\n",
    "            data = json.loads(req_t.text)\n",
    "            if 'response' in data:\n",
    "                articles.extend(data['response']['docs'])\n",
    "        return articles\n",
    "\n",
    "    def get_text(self, articles):\n",
    "        for a in articles:\n",
    "            try:\n",
    "                article = Article(a['web_url'])\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "                a['summary'] = article.summary\n",
    "                a['keywords_1'] = article.keywords\n",
    "                a['text'] = article.text\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "\n",
    "def get_tokens():\n",
    "    corpus = \"\"\n",
    "    with open(\"out/nyt_articles.json\") as data_file:\n",
    "        data = json.load(data_file)\n",
    "    for article in data:\n",
    "        corpus += article['text']\n",
    "    lowers = corpus.lower()\n",
    "    no_punctuation = lowers.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    toker = nltk.RegexpTokenizer(r'\\w+')\n",
    "    tokens = toker.tokenize(no_punctuation)\n",
    "    count = nltk.Counter(tokens)\n",
    "    print(count.most_common(10))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    count = nltk.Counter(filtered)\n",
    "    print(count.most_common(10))\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        # stemmed.append(nltk.PorterStemmer().stem(item))\n",
    "        stemmed.append(nltk.WordNetLemmatizer().lemmatize(item))\n",
    "    count = nltk.Counter(stemmed)\n",
    "    # print(count.most_common(10))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def get_article_token():\n",
    "    article_token_dict = {}\n",
    "    toker = nltk.RegexpTokenizer(r'\\w+')\n",
    "    with open(\"out/nyt_articles.json\") as data_file:\n",
    "        data = json.load(data_file)\n",
    "    for article in data:\n",
    "        text = article['text']\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        tokens = toker.tokenize(no_punctuation)\n",
    "        article_token_dict[article['_id']] = \" \".join(tokens)\n",
    "    return article_token_dict\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    toker = nltk.RegexpTokenizer(r'\\w+')\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = toker.tokenize(no_punctuation)\n",
    "    stems = stem_tokens(tokens)\n",
    "    return stems\n",
    "\n",
    "\n",
    "def get_promise_token():\n",
    "    promise_token_dict = {}\n",
    "    token = nltk.RegexpTokenizer(r'\\w+')\n",
    "    with open(\"out/promises2.json\") as data_file:\n",
    "        data = json.load(data_file)\n",
    "    for promise in data:\n",
    "        if promise['promise_description']:\n",
    "            text = promise['promise_description']\n",
    "        else:\n",
    "            text = promise['promise_title']\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        tokens = token.tokenize(no_punctuation)\n",
    "        promise_token_dict[promise['promise_title']] = \" \".join(tokens)\n",
    "    return promise_token_dict\n",
    "\n",
    "\n",
    "def google_search(search_query):\n",
    "    search_sentiment_result = []\n",
    "    search_query = search_query.replace(\" \", \"+\")\n",
    "    query = \"https://www.google.com/search?q=\" + search_query\n",
    "    r = requests.get(query)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    for s in soup.find_all(attrs={'class': 'st'}):\n",
    "        search_sentiment_result.append(sentiment_analysis(s.text))\n",
    "\n",
    "    my_list = {i: search_sentiment_result.count(i) for i in search_sentiment_result}\n",
    "    print(max(my_list.items(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "\n",
    "def sentiment_analysis(c):\n",
    "    blob = TextBlob(c)\n",
    "    for sentence in blob.sentences:\n",
    "        polarity = sentence.sentiment.polarity\n",
    "        if -1.0 <= polarity <= -0.5:\n",
    "            return \"Negative response\"\n",
    "        elif -0.5 < polarity <= 0:\n",
    "            return \"Slightly Negative response\"\n",
    "        elif 0 <= polarity < 0.5:\n",
    "            return \"Slightly Positive Response\"\n",
    "        else:\n",
    "            return \"Positive Response\"\n",
    "\n",
    "def match_articles(promise):\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[promise: promise + 1], tfidf_matrix)\n",
    "    single_array = np.array(cosine_sim[0])\n",
    "    article_array = single_array.argsort()[-6:][::-1]\n",
    "    only_articles = [s for s in article_array if s > 1]\n",
    "    promise_statement = all_tokens[promise]\n",
    "    print(\"The promise made by the candidate was: \\n\", promise_statement)\n",
    "    count = 1\n",
    "    for x in only_articles:\n",
    "        print(count, all_tokens[x] + \"\\n\")\n",
    "        print(sentiment_analysis(all_tokens[x]))\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Political Promise Evaluation (PPE)\n",
    "#### NLP CS6120\n",
    "##### By: Bhanu Jain, Rohit Begani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inspirations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/trumptracker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/politifact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem \n",
    "#### Track performance of political leaders throught continous evaluation of promises made during election campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Data\n",
    "\n",
    "## Promises\n",
    "- Politifact \n",
    "\n",
    "## Promise Evaluation\n",
    "- Twiiter \n",
    "- NYT\n",
    "- Google Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Promises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"out/promises.json\") as data_file:\n",
    "    p_data = json.load(data_file)\n",
    "\n",
    "len(p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'promise_description': '',\n",
      " 'promise_tag': ['Contract with the American Voter - 100 Day Plan'],\n",
      " 'promise_title': 'Propose a Constitutional Amendment to impose term limits on '\n",
      "                  'all members of Congress'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(p_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeal and Replace Obamacare Act:\n",
      "fully repeals obamacare and replaces it with health savings accounts the ability to purchase health insurance across state lines and lets states manage medicaid funds reforms will also include cutting the red tape at the fda there are over 4000 drugs awaiting approval and we especially want to speed the approval of lifesaving medications\n",
      "\n",
      "Build a wall. Trump's campaign began with a promise to build a wall across the United States' southern border and deport the country's 11 million undocumented immigrants.:\n",
      "build a wall trumps campaign began with a promise to build a wall across the united states southern border and deport the countrys 11 million undocumented immigrants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "promises = get_promise_token()\n",
    "for k,v in promises.items():    \n",
    "    print(k+\":\\n\"+v+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't talk crap..\n",
      "\n",
      "At least stand in respectful solidarity with the majority Syrian population &amp; their demands. You… https://t.co/7RJZtkqNqW\n",
      "RT @RawStory: #RussiaRussiaRussia. #Trump has turned into #JanBrady. https://t.co/vDQQH3QPQ1\n",
      "RT @ToDropADime2: If your Husband or Wife Lied to you 5 times a day, would you stay with them?\n",
      "\n",
      "#Trump Lies 5.5 times a day......\n",
      "RT @haaretzcom: Trump Tweets anti-Semitic, Conspiracy Theorist Website Boasting About His Accomplishments\n",
      "https://t.co/grh6SN1PVx\n",
      "#Trump #A…\n"
     ]
    }
   ],
   "source": [
    "tweets = TwitterMiner().login().search.tweets(q=\"#trump\", count=5)\n",
    "for tweet in tweets['statuses']:\n",
    "    print(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NYT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In Damac, Mr. Trump\\u2019s company has found a like-minded partner, one that has emblazoned its name across skyscrapers.\\n\\nAdvertisement Continue reading the main story\\n\\nThe company\\u2019s founder, Hussain Sajwani, has a net worth estimated at around $4 billion, earning him the nickname \\u201cthe Donald of Dubai,\\u201d \"\n"
     ]
    }
   ],
   "source": [
    "nm = NewsMiner()\n",
    "articles = nm.get_articles(\"Donald Trump\", \"20170730\", \"20170830\")\n",
    "nm.get_text(articles)\n",
    "print(json.dumps(articles[0]['text'][:300],indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary and Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"He and his family spent New Year\\u2019s Eve with Mr. Trump at his Florida resort, and he attended the president\\u2019s inauguration.\\nIn an interview with The Times last year, Mr. McLoughlin, the Damac spokesman, said Mr. Trump had visited Damac Hills several times.\\nBuilding anything in Dubai invariably involves migrant workers, in transactions that often deviate from the law.\\nIn response, the government has made it permissible for migrant workers to change jobs while outlawing payments to recruitment firms.\\nAl Arif, the company supplying workers for the Trump course at Damac Hills, houses its workers in isolated apartment complexes in the desert.\"\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(articles[0]['summary'],indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wages', 'reading', 'dubai', 'course', 'golf', 'trump', 'workers', 'main', 'donald', 'hills', 'damac', 'late', 'mr', 'migrant']\n"
     ]
    }
   ],
   "source": [
    "print(articles[0]['keywords_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Articles Len = 130\n"
     ]
    }
   ],
   "source": [
    "articles = get_article_token()\n",
    "print(\"Total Articles Len = %s\" %(len(articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 59934a3d95d0e0246f1ffdc7\n",
      "Text: the exodus began monday when merck s chief executive said he was resigning from mr trump s manufacturing council citing the president s tepid early statement on the violence in charlottesville where white nationalists staged a weekend march that turned violent on tuesday the leaders of a labor group\n"
     ]
    }
   ],
   "source": [
    "print(\"Key: %s\" %list(articles.keys())[0])\n",
    "print(\"Text: %s\" %articles[list(articles.keys())[0]][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Article Sentiment \n",
    "\n",
    "## Article Matching \n",
    "- Bag of Words\n",
    "- Tf-Idf\n",
    "\n",
    "## Sentiment Analysis\n",
    "- Naive Bayes\n",
    "- Pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Summary Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aspect Based Senitment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Whats Next"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ppe",
   "language": "python",
   "name": "ppe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
